# Нейронные сети([Слайды](https://www.dropbox.com/s/ywn9xoxeyy7250b/Lecture%203%20-%20Neural%20Networks%20-%20annotated.pdf?dl=0))

Нейросети имают весьма условное отдношение к настоящим нейронам, они лишь вдохновлены ими

![](images/2020-09-26-11-06-59.png)

Как собрать нейросеть которая вычисляет число на кртинке:

1. Развернуть матрицу пикселей картинки в плоский вектор(32*32*3)
2. Подключть её к 10 нейронов и подобрать веса для связей

Если так сделать получится система равноценная **Линейному классификатору**

## Линейный классификатор

![formula](https://render.githubusercontent.com/render/math?math=y=x*w%2Bb)

b - смещения(это аналог константых весов w0)

## Градиентный спуск

![](images/2020-09-26-11-26-29.png)

1. Выбираем случайную току
2. Находим градиент в этой точке
3. Делаем шаг в направление анти-градиента
4. Повторять шаги 2, 3, пока не сойдмся в точку, которая является локальным минимумом

?Для простых случаев даже есть гарантия схода в глобальный минимум(ковекс)?

Размер шага - ![formula](https://render.githubusercontent.com/render/math?math=\lambda) (гиперпараметр)

Чем меньше размерность тем выше вероятность глобальности минимума

## Softmax

Мы хотим чтобы в результате для каждого класса было число 0..1, это его вероятность. Сумма вероятностей должна быть = 1

![formula](<https://render.githubusercontent.com/render/math?math=p(C=0|x)=\frac{e^{y_0}}{e^{y_0}%2Be^{y_1}%2B...%2Be^{y_n}}=\frac{e^{y_0}}{\sum_{i}^{n}{e^{y_i}}}>)

Где C = 0 - значит что это вероятность 0-ого класса

Softmax - прямое расширение логиситческой функции для многоклассовой классификации

Это не единственный варант, но самый общепринятый

## Принцип максимального правдаподобия

1. Для каждого примера посчитать вероятность правильного ответа
2. Умножим все эти вероятности между собой

gt - ground truth(правильная метка)

![formula](<https://render.githubusercontent.com/render/math?math=p(data)=\prod_{s}{p(c=gt_s|x_s)}>)

Нам надо максимизировать эту метрику по w и b

Обычно приминяетя аналогичный метод который проще считать(Negative Log-likelihood):

![formula](<https://render.githubusercontent.com/render/math?math=-ln{p}(data)=-\sum{ln{p}(c=gt_s|x_s)}>)

![formula](<https://render.githubusercontent.com/render/math?math=-\sum_{s}{ln{\frac{e^{(wx_s+)gt_s}}{\sum_{i}{e^{(wx_s+b)_i}}}}}=L>) - это loss

Эту страхомань мы и хотим минимизировать

## Регуляризация

![formula](<https://render.githubusercontent.com/render/math?math=L=-\sum_{s}{ln{\frac{e^{(wx_s+)gt_s}}{\sum_{i}{e^{(wx_s+b)_i}}}}}%2B\lambda+R(w,b)>)

L2 регуляризатор:
![formula](<https://render.githubusercontent.com/render/math?math=R(w,b)=||w||^2_2%2B||b||^2_2>)

Ругуляризатор позволяет сделать функцию более плавной и уменьшить вероятность overfit

![formula](https://render.githubusercontent.com/render/math?math=\lambda) - гиперпараметр

![formula](https://render.githubusercontent.com/render/math?math=||w||_2^2=\sqrt{w_00^2%2Bw_01^2+...}^2)

## Снова градиентый спуск

![formula](https://render.githubusercontent.com/render/math?math=\vec{w}=\vec{w}-\nu\vec{\nabla_w}L)

![formula](https://render.githubusercontent.com/render/math?math=\vec{b}=\vec{b}-\nu\vec{\nabla_b}L)

![formula](https://render.githubusercontent.com/render/math?math=\nu) - размер шага(learning rate)

Как посчитать градиент?

**Finite differences(численный градиент)**

![formula](<https://render.githubusercontent.com/render/math?math=L`(x)=\frac{L(x%2B\epsilon)-L(x-\epsilon)}{2\epsilon}>)

Рабочий, но не эффективный вариант, зато лекгко реализуется

**Analytic gradient(аналитический градиент)**

![](images/2020-09-26-12-06-37.png)

## Стохастический(случайный) градиентный спуск

![formula](<https://render.githubusercontent.com/render/math?math=L=-\sum_{j}{ln{p}(c=y_j|x_j)}>)

Берём minibatch(часть примеров) и оптимизируем для неё

## Нейронные сети

Просто берём и добавляем больше одного слоя линейных классификаторов!

А нет, не просто, между слоями надо добавить функции активации(non-linear function)

![](images/2020-09-26-12-35-59.png)

ReLU круче TanH

На выходе у нас получается та же функция

![](images/2020-09-26-12-37-52.png)

![formula](<https://render.githubusercontent.com/render/math?math=L=-\sum_{j}{ln{p}(c=y_j|x_j)%2B\lambda%20R(w)}>)

![formula](https://render.githubusercontent.com/render/math?math=\vec{w^1}=\vec{w^1}-\nu\vec{\nabla_{w^1}}L)

![formula](https://render.githubusercontent.com/render/math?math=\vec{w^2}=\vec{w^2}-\nu\vec{\nabla_{w^2}}L)

![formula](https://render.githubusercontent.com/render/math?math=\vec{w^3}=\vec{w^3}-\nu\vec{\nabla_{w^3}}L)

**Как посчитать** ![formula](https://render.githubusercontent.com/render/math?math=\vec\nabla_wL) ?

Колличество и параметры слоёв - гиперпараметры

## Граф вычислений

![formula](<https://render.githubusercontent.com/render/math?math=L=-\sum_{j}{ln{p}(c=y_j|x_j)%2B\lambda%20R(w)}>)

![](images/2020-09-26-12-43-47.png)

Нам нужно найти численные градиенты по всем входам графа

Так умеет алгоритм back prpagation(обратного распространения ошибки)

## Back propagation(обратное распрострнаение ошибки)

![](images/2020-09-26-12-46-46.png)

Наш граф это большая очень сложная(вложенная) функция

![](images/2020-09-26-12-54-19.png)

Очень интересно, ничего не понятно(ну почти), [пресмотреть](https://youtu.be/kWTC1NvL894?t=4331)

## Общая схема вычисления градиента

![](images/2020-09-26-12-59-05.png)

## В коде

**forward**

```python
x = 1
w1 = 2
w0 = 1
a = x*w1
b = a + w0
c = np.exp(b)
f = 1 + c
```

**backward**

```python
df = 1
dc = df
db = np.exp(b)*dc
dw0 = db
da = db
dw1 = x*da
dx = w1*da
```

## Уточнения

Нода Max - зная где был максимум отдаёт градиент ему, а дргуим входам 0, если одинкаовые можно выбрать кому отдават случайно или делить попалам и отдать обоим

Если параметр участвует в вычислениях >1 раза и ему возвращают градиент разные ноды, их просто надо сложить
